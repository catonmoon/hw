{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "<center> Автор материала: Павел Нестеров (@mephistopheies).\n",
    "\n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.5.2\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.14.1\n",
      "scipy 0.19.0\n",
      "pandas 0.22.0\n",
      "matplotlib 2.0.2\n",
      "sklearn 0.18.1\n",
      "\n",
      "compiler   : GCC 5.4.0 20160609\n",
      "system     : Linux\n",
      "release    : 4.4.0-116-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "Git hash   : b91eddb1dd579bea61cf24f66ebb25071ca7369b\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ios', 'android', 'html', 'javascript', 'java', 'jquery', 'c#', 'c++', 'php', 'python'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\textbf{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\textbf{x}, y}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\textbf{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\textbf{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\textbf{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\textbf{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font> В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ae664649714a67995ceb35bc3c1921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f7ba14940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 21.13\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy_sum = 0\n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = set()\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    elif sigma > 0.9:\n",
    "                        # тестовый набор, предсказываем тег\n",
    "                        predicted_tags.add(tag)\n",
    "                        \n",
    "                if n >= top_n_train:\n",
    "                    accuracy_sum += len(predicted_tags.intersection(tags)) / len(tags.union(predicted_tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return accuracy_sum / (total - top_n_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e54a8c0c8164151b4ad0c5adac83bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(\\textbf W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     lmbda=0.01,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy_sum = 0\n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = set()\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "                    \n",
    "                    w_filter = set()\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            \n",
    "                            if word in w_filter:\n",
    "                                self._w[tag][self._vocab[word]] -= -learning_rate*dLdw \n",
    "                            else:\n",
    "                                w_filter.add(word)\n",
    "                                self._w[tag][self._vocab[word]] -= -learning_rate*(dLdw -lmbda*self._w[tag][self._vocab[word]]) \n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    elif sigma > 0.9:\n",
    "                        # тестовый набор, предсказываем тег\n",
    "                        predicted_tags.add(tag)\n",
    "                        \n",
    "                if n >= top_n_train:\n",
    "                    accuracy_sum += len(predicted_tags.intersection(tags)) / len(tags.union(predicted_tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return accuracy_sum / (total - top_n_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21900bd51b04bef842b7e008f4c07bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f8cc1e898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$ V\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy_sum = 0\n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = set()\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "                    \n",
    "                    w_filter = set()\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            \n",
    "                            if word in w_filter:\n",
    "                                reg = 0\n",
    "                            else:\n",
    "                                w_filter.add(word)\n",
    "                                w_tmp = self._w[tag][self._vocab[word]] \n",
    "                                reg = lmbda * (gamma * 2 * w_tmp + (1 - gamma) * np.sign(w_tmp))\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*(dLdw - reg) \n",
    "                            \n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    elif sigma > 0.9:\n",
    "                        # тестовый набор, предсказываем тег\n",
    "                        predicted_tags.add(tag)\n",
    "                        \n",
    "                if n >= top_n_train:\n",
    "                    accuracy_sum += len(predicted_tags.intersection(tags)) / len(tags.union(predicted_tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return accuracy_sum / (total - top_n_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f15a81158c544b0b1be54638d29724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.59\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD3CAYAAAAE2w/rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XeclNW5wPHf7LKFhV1YBJZeBDlSVIqNXoyKMdGruSYaERPECwlGsMJFiBo1WbAhgmLbi3Il8ULElhiJskgX6dIOIk2kSN9dtrDt/jGzU3Y6O++8884838+Hz+dts/PssPPMmfOe8xxbdXU1QgghrCfJ7ACEEEKcH0ngQghhUZLAhRDCoiSBCyGERUkCF0IIi6oXjSc5dqxQhroIIUSYmjXLtAU6Ly1wIYSwKEngQghhUZLAhRDCoiSBCyGERUkCF0IIi5IELoQQFiUJXAghLEoSuBBCWFTAiTxKqRQgD+gApAFPAweAOUAFsAsYrbWuMirAUblL6Ngyi/tuvYSkJBuNGqQa9VRCCGEpwVrgI4ATWuuBwHBgFvA48Cet9QDsSf1Go4Lbf6QQgL2HC3ho9koeeHmFUU8lhBCWE2wq/QJgoWPbhr3VvRFoopSyAZlAuVHBtWqa4XWs7FwlaanJRj2lEEJYRsAWuNa6SGtdqJTKxJ7IpwDfAjOBHUAOsNSo4FLqJZOc5FkK4N3Pdxn1dEIIYSlBb2IqpdoC+cA8rfV84CVgoNb6YuAd4HkjA+zaPttjf8WWw0Y+nRBCWEbABK6UygEWAxO11nmOwyeBAsf2ISDb12Mj5fKLm3sdO11UZuRTCiGEJdgCLWqslHoJ+BWw0+3wVCAXe3/4OeBerfW+QE9S13Kyo3KX0PKCDA6fKHYem3Rnb7q0bVyXHyuEEDEtWDnZgAk8UiJVD3xU7hKP/bxJwyLxY4UQIiZJPXAhhIhTlkrgbzw6hKu65Tj3o/HtQQghYpWlEnhyUhJjburu3F+vj5kYjRBCmMtSCby2svJKs0MQQgjTWDqBv/WPHdKNIoRIWJZM4N07uIaeb9gl3ShCiMRkyQQ++ueufvB/rN5vYiRCCGEeSybwrIwU5/a+I4XMl/ooQogEZMkEbrPZqJfsCv3zdQdNjEYIIcxhyQQO8Ozv+nrs7/7hjEmRCCGEOSybwBs1TPOYSl9ZadiiQEIIEZMsm8Br3HB1OwDe/fe3JkcihBDRZfkEvm3vSQAOHiuiqMSwxYGEECLmWD6Bp9Rz/QqTX19jYiRCCBFdlk/gV1zsKm7VuGGaiZEIIUR0WT6B9+vRwrl98FiRiZEIIUR0WT6BN6yfwsRf9wKgT5dmJkcjhBDRUy/YBUqpFCAP6ACkAU8DvwZqmr4dgDVa69uNCTG4jHT7zMyyikoO/lhEm+YNzQpFCCGiJpQW+AjghNZ6IDAcmKW1vl1rPQS4BTgNPGBciMGlpth/ja17TvLHvLVmhiKEEFETtAUOLAAWOrZt2BczrvEk8LLW+nCkAwtHSrLn51BxaQUZ6aH8akIIYV1BW+Ba6yKtdaFSKhN7Ip8CoJRqDlwDzDU0whCkpSZ77N83Y5lJkQghRPSEdBNTKdUWyAfmaa3nOw7/JzBfa236sjgN0lO4fVhns8MQQoioCprAlVI5wGJgotY6z+3UT4BPjQosXNdd2Y7nx/V37stKPUKIeBdKR/FkIBuYqpSa6jh2A6CAPUYFdj6yM10TeU4XnfPYF0KIeGOLRkv12LHCqDWHR+UucW67VysUQgiradYs0xbovOUn8gRSWSUlZoUQ8SvuEvhTo69ybv9T1ssUQsSxuEvgrZs2cG4vWr7XxEiEEMJYcZfAAW4b0snsEIQQwnBxmcBvuLq92SEIIYTh4jKBAyQn2W/e1oyyKa+ooqSsItBDhBDCUuK2YEi7nEz2Hi6g9Fwl9dPqMea5pR7nZYihEMLq4rYFvvdwAQDLt/iusyUzNYUQVhe3Cbx9i0wAMtLqcfxMidf5LzcfinZIQggRUXE3E7PG5t3HeWnhloDXSDeKECKWJexMzJqbmEIIEa/iNoF379jE69iAS1oy+4FBJkQjhBCRF7cJ3GbzboGPurEr9dNcA2/cC18JIYTVxG0CB7i6W07Qaw7+WBSFSIQQIvLiOoHfOvjCoNfIIshCCKuK6wTuvqBDVkaKc3to79Ye11VUStlZIYT1xHUCT05y/XoP397LuX3XdYpn7nWVnf3L/26IalxCCBEJAafSK6VSgDygA5AGPA2sAd7AvsxaMjBSa/2dsWGev7cmDqWsvJL0VM9fteUFrrKzNbM2hRDCSoK1wEcAJ7TWA4HhwCxgOvCu1noQMAW42NgQ68Zms3kl7xrjbukBwMXtGkczJCGEiIhgxawWAAsd2zagAugPbFFKfQ7sA8YbFp3BzpbaqxPuPHDa5EiEECJ8AVvgWusirXWhUioTeyKfgr075ZTW+ifAAWCi4VEapF+PFs7topJyEyMRQojwBb2JqZRqC+QD87TW84ETwEeO0x8DlxsXnrHqJbt+/ftfWm5iJEIIEb6ACVwplQMsBiZqrfMch1cAP3VsDwK2GReeEEIIf4K1wCdjH20yVSm1VCm1FHgIGKmUWoX9xuafjQ3RWK8/MsS5XVUlNcKFENYR8Cam1no8vm9SXmtMONHn3o2yYdcxLr+4uYnRCCFE6OJ6Ik+4Xvlgq9khCCFEyCSBA0N6tjI7BCGECJskcGD4Ve0AaNEkw+RIhBAidJLAgcyMVACOnCym4Ow5k6MRQojQSAIH0lOTndsTXl6BPnDKxGiEECI0ksDxXr1n2vyNnCwoNSkaIYQIjSRwPx5+ZZXZIQghRECSwB1efXCw2SEIIURYJIE7pKUmkzdpmNlhCCFEyCSB1yJJPH6VV8jSeSK+SAIP4PjpErNDEBHy9c4fGfPcUrZ8d9zsUISIGEngATw6Z7XZIYgI+XTNfgA+X3/Q5EiEiBxJ4CIhHHN8m9q656TJkQgROZLAfXAfkSIr9cSHmuXzAKqrpWywiA+SwH1Ic5uZeej4WRMjEZFQO2G/85k2KRIhIksSuB/JSfbZmXM+lBKzVufe+gb4ctMhkyIRIrIkgfvRIN2+1sXpIiluZUW5/7ueUblLqK6uprBY/g9FfAq4Ig+AUioFyMO+Gn0a8DTwPfAJ8K3jsle11u8ZFKMpHvhlT56c+7XZYYjztOvgGQDumZbPgEtbmhyNEMYImsCBEcAJrfVdSqkmwCbgT8ALWuvnDY3ORO1bZDq3K6uqSE6SLyuxrOxcJd8dOkO3Dk28bjyv2HIYsP+f7j9SaEZ4QhgilAS+AFjo2LYBFUAfQCmlbsbeCp+gtY7bd8aKLYcZ3LO12WGIAH73wpcA9OzclKu75/i8xj15y4eyiAdB/4K11kVa60KlVCb2RD4FWAs8orUeBOwBHjc2THO9/S8ZtRDLtu9zje3etPs4H63c5/O6R+7oReOG9sU7SsoqoxGaEIYKqQmilGoL5APztNbzgUVa6/WO04uAXgbFZyr3llxJWQVb95yQMcQxZvcPZ3jub5s8jtUM/WyeXd/jeJPMNM44Vlz65rsT0QlQCAMFTeBKqRxgMTBRa53nOPyZUupKx/Y1wHqfD7a4e3/Wzbk97sVlvPB/m5m9SIYVxpI/z/P/p9e7SzOP/cYN06j5/H3jk+1GhiVEVITSAp8MZANTlVJLlVJLgQeBFx3b/bGPTIk7tVfqAdiw65gJkQhfPv1qv8d+7ti+Hvtd2jTmL2Oudu6npiRxY9/2zv0fTxUbG6AQBgt6E1NrPR4Y7+NU/8iHE3tS6iVJGdIYtSD/O4/95o3rc/9/XsrMhVsAuLBVFg0zUpznbTYbPTo24R+r7Yl/0mtrpHywsLRQRqEktGlj+/LgrJVmhyFqqah0fai2vCCDP/ziUsA+CuWeG7vSuU0jshrYb1i+OXEoNd+lVLvsaIcqhGFkHFUQjRumMWXk5Tw/LiG+cFjGfz271Ln95KgradEkw7nf/5KW5GS79pNsNo/usEfvcN1zX731iLGBCmEgSeAhuLBVFtmZac79c+UyBM1Mpec8a5vUSw7vz7hL28bO7S82SH1wYV2SwM/DsTOlVFWFN5ywSoYfRszvX1jm3H71ofAXo05KcrXGC85KnRRhXZLAw5DpuCH2+FtrGT09n/mf7wrpcXsOFTB6Wj6jcpcYGV7cO3ziLO8v87xxmZaS7OfqwCbd2RuA42dK5f9FWJYk8DB0atUIcLWmP193MKQRKk+/s865XXZOul/O12NvfMUnq1xDB+tyX6JT66xIhCSEqSSBh6GRYxq2uzHPLeXfX3/v9zGLlu3x2F+2WWpRn4812z1vNo7+WVeP+xLhql0HZdteWWpNWI8k8DDc1L+jz+N//eJbn8cBPl61L+RrhX+vf+Q5c7Jfj7qXiH1z4lDn9vPvbQpwpRCxSRJ4GIK1+A4cLWRU7hI+WrE3ShElhsfeWGPIz03yMdNWCCuRBF4Hb7m14BYs3c0T/2NfAOIDRwLf6Dbt/u7hyrl9sqA0ShHGh8MnjJvyPvmuPob9bCGMJgk8TE0bpTu33SeHfLrmgNe1L7//jXPbvZ74w6+sMii6xPCHX1wSsZ/VuXUj5/bitd7/h0LEMplKH6bcMX15/r1N/PanFwe87pVFruR9UZtGHueu6d3GkNjindF1Sz5auY/rrmxn6HMIEUnSAg9TUpKNR+7oRdNG9lrT02tVwKuxTru6T0b9tCsAd1xzEWCf/Sd1xUMTjVmv99xo//8pLqsIcqUQsUUSeB01bey5aMDP+rX3uibHUaejj3LVp75nWr7X2o3C267vTxv+HJ1aNwp+kRAxSBJ4BOSOuZqruuUwc/xAbh3UyePciOu6OLebZKV7nHs8b21U4rMyHYUE7l4IK9wSCUKYSRJ4BDTPzmDMTd1pWD/F69ygy1r5fdypwjIjw4oLhcX2bynXXdE2Ks93+KQs8iCsQxK4wWpXyvuPgb4nAwnfNn93HICcWutbRlr7nEwA1m4/aujzCBFJARO4UipFKTVPKbVcKbVWKXWT27lfK6VWGx+i9dx3q32Y229u8B6pclP/jjx0e0/n/t7DBVGLy2qqq6s5U2SvFnhpp6aGPtdV3ewLWH+8ap/HYhFCxLJgLfARwAmt9UBgODALQCnVC7gHkKlsPvTu0oy8ScP8dp9079DEuf3U2+t8XiOg1K3wl686NJHkXiP8pHRtCYsIlsAXAFMd2zagQil1AfBnYIKRgSWSgmKpSe3LsdMlzu1wF20I14WtXNUJJ82RL5bCGgK+K7TWRVrrQqVUJrAQezJ/C/uq9IVRiC9u3Xmta3TKhJkrTIwkNh08VuQsTSCE8C1os0Yp1RbIB+YB3wIXAa8CfwO6KaVmGBphnLqmTxs6ubX6pMyspz++5RpiGa3a3TPHD3Ruy0QrYQXBbmLmAIuBiVrrPK31Wq11d631EOB2YLvWWrpSztON/To4t+d+upNz5ZVMmLmcdTt/NC+oGPToHb2j8jzuw0BlVqawgmAt8MlANjBVKbXU8c/Y8VwJpPY08YVLv6OguJxXPthqUkSx57WHB5NSL/qjXbfukQUeROwLWMxKaz0eGO/n3D7gagNiShida03hLncbvlZdXe1R7TCRuBcCS6l3fmte1tVrH21zDi0UIlbJRB4TNclKZ47bqupfbnL1g98zLd+MkExXVVXtUQhMCOGfJHCTpQZYVb2svJLFaw+wdkfizA58+187TX3+537fz9TnFyIcksBjQK6fkrS/e/5L/rZkN3M+3Mao3CUJUb1w+ZbDzu3ZDwyK+vPXZaFkIaJNEngMaN44tPvC97+0nJ37TxkcTWwY0qs19dOiv96I+32H42dKAlwphPkkgceIUEdaTP/rRoMjMY/72OstjiJWZlqzLXG6roQ1SQKPEc86+l5bXpDBXderIFfHp/IK1yic6WPN64u+eYC9YmTb5g1Ni0GIUEgCjxFZGan8ZczV/PHuKxjaqzWzJthnBbZvkclrDw+hx4WuAlj6wCnOlpZ7LT6wetsR/i9/d1TjjqQPV+51biclmTeE8mRBKeA5KkiIWCSLGseQnGzXyjAZ6Skei/g+cNtlzqGF0+bbu1H692jBPT/rBsDDr6zkZIG9it5lnS5AtcuOVtgR8+ma2FgVvsRRBXHTbvO7cYQIRFrgFuFrUs/KrUeorq4mf8NBZ/IGV4K3qmaN04NfZKDeXYytPS5EpEgCt7gV3xxm3uJdZocRUZPu7GPq87drnuncljUyRSyTBG4hb00c6nXsf/5p7sSXSKnpdwZoWN/cnr1WTRs4t90XlRAi1kgCtxCbzUbepGHkTRpG9w7efdyvPzLEuW21cqhz3WZgmlX/xN2VXZsDUFwa/5OnhHVJAreo399yidcx91VrDhwtimY4dVZcai/felGbRkGujI61O+wlfR+ds5on58rCEiI2SQK3qNqzFP9jgOdq90/O/ZofjlkniZ91JPA7fnKRyZF423+kkFnvf4M+kBizYIV1SAK3sKl3X87twzqTN2kYNzkSeGqK6790qtuqNrHu6MliAJpkmjsCpcbkuzxvpG7YdYxp8zdarmtKxDdJ4BbWsWUW113ZzuPYS/cP9Ni3wigK95ojDTNSAlwZPY0bpPo8vmHXcc6WljMqdwmbZZy4MJkk8DiTlpLM5BGu1uPo6bFfV7zgrOtGYVKMLGKR7qeQ1qnCUmYs2AzASwu3RDMkIbwEHK+llEoB8oAOQBrwNLAbeB2wYV/keLTWWhYQjCGda90IPHqq2GOWZ6yZPn+D2SF4cV8fM6VekrNOy/zPvzUrJCG8BGuBjwBOaK0HAsOBWcCfgcla6/6Oa35uYHziPN3sdlPzv19bY2IkwZ1zJMfm2bG13OorDw5izkODee3hIV4fijUSoUa7iF3BEvgCYKpj2wZUAL/QWi9TSqUCLYAzBsYnztPNtUalWMFT91xldgge0lPrOVdMmnRnb5/XLFq+h+OnSxiVu8T5T250imgJmMC11kVa60KlVCawEJiita5USrUHtgFNgc1RiFOch5njBwa/yGRrth9xbpux+nyo/PXN52/4gUfnrPY4tiNBFt0Q5gv6jlFKtQXygXla6/kAWuv9WuuLgDnAC8aGKM6Xez9uSVls3qZ4/aPtZocQso4tXTVS7h7uv2b7c3/bBMDBY0Vs/FYWaBbGCXYTMwdYDNyntf7Ccewj4CGt9bdAIVAV4EeIGHH4RDEXtsoyOwy/ftKnjdkhBDX17iuorKoiyWajorKat/+l/V77zDvr+O5QAWAvceA+S1aISAn2VzUZyAamKqWWKqWWAtOAuUqpfGCk4xoR455+Z53ZIXhx7yvu26OFiZGELjkpCZvNFrS7pyZ5AxTH6LcfYX0BW+Ba6/HAeB+n+vs4JmJQjwubsHXPScCeMH3VFTeL+wiODi0yA1wZm96cOJSpb37Fg7/sSVpqMumpyfzXs0u9rtuw6xhDeraOfoAi7sn3ujh366ALndsfrtgb4MroqykYBb4XrIh1STYbz9x7NRc0Sqdh/RS/3STvBOhqEaIuZEm1ONe0kWts9aHjZ02MxNu7/46vhSgApo/ty94jhVzUphGfrjnAv9d9T1qK+eVxRXySFnica1g/hWG97V/fY7Uv9rc3XGx2CBHTtHF9rri4OY0bpnFp5wsAKCuXRSGEMSSBJ4CL2jQGYPu+2Byf3LW99RZgDkWnGB71I+KDJPAE4L5IQkVlbIz6PFPkWoS5aePYmkIfKemprh7KvYcLAlwpxPmRBJ4AmmS5amy/v2yPiZHYFRaf44FZK80OI6qeensdo3KXsHTjD2aHIuKIJPAEc+REsdkhMPv9b8wOIWpauy2QDPDOZzIiRUSOJPAEcWkn+w21TTGwCMGug4lT/+zuOLpBK2KPJPAEEavVCd98dKjZIRiqc+tGzg9PISJNEniCiMWZjq8+OJikJOtN4AnXhNsu89iPlRvJwvokgScI95mOlVWxkUDSUhNngssrDw5ybpeek3HhIjIkgSege6cvZcWWw6Y8d1WCLnbgPqRwzyEZUigiQxJ4Aul1UVPndt4/d5gSQyyMgjHLlV2bA7G9cIWwFvlLSiDu48HBnNbw2VJ7BcLuHZtE/bnN1qm1fULVWVlHU0SIJPAE8vN+HTz2R0/L97qmqKSc4lLjEswPx+wFtRIxidUUtZLaKCJSJIEnkKwGqYy75RKPY9/sOeGxf/9Ly7lvxnLDYqiZyJKIq7mnptjfbuUVsXETWVifJPAE00c146nRrtXfX/67a1ZkNIe3ZWakRu25YsXpwnMAfLb2gMmRiHgRbE3MFCAP6ACkAU8DB4CXgUqgDBiptT5qbJgiktynd1dUVjEqdwkA2ZlpzuNfrD/INQauU3n/Ly4JflGcaeYo2nX0VInJkYh4EawFPgI4obUeCAwHZgEvAX/QWg8B3gcmGhqhMMQlF3rPDjxV6KoQaPRiC1kNEq8F3qaZ64Nz694TAa4UIjTBEvgCYKpj2wZUALdrrTc5jtUDSg2KTRho9M+6Bjzf3IASr+59v1ZcQq2umme7XtMX3ttsYiQiXgRM4FrrIq11oVIqE1gITNFaHwZQSvUD7gNeND5MEWmZGak89Kuefs//eDryX/NrRreoto0j/rOtoPaHViLeyBWRFfQmplKqLZAPzNNaz3cc+xUwB7hRa33M2BCFUbp3bMKbE4cy8noVlef76xffAqC/Px2V54tFM+4f4Nz+arvcOhJ1EzCBK6VygMXARK11nuPYCOwt7yFaa/NXBxB1kmSzMaRXa24Z2NG5X6M6whN93FehT1RZGak0rJ8CxOeiziK6gq1KPxnIBqYqpaYCyUAPYD/wvlIK4Eut9eOGRikM9/P+Hbm0U1OaNk7ntY+2sXXPSc6WVjiTTSQ9/psrIv4zrcR9ElNZeaWsWi/OW8AErrUeD4yPUizCZO0dJWdzsjPYykmOnymJWAI/5zb7sHWzBgGujH8P/qonz79nHwewbPMhrr28rckRCauSiTzCS02y/dPcdRH7mT+6jX2ul5zYf3budWB27j9lYiTC6hL7nSR8Uu0iP0rkyEl7FcKsjMh3yVjRkF6tAdj47XFKyipMjkZYlSRw4aVfj5bO7UgllwM/FgHQw8cEokSUkebqvRz34jITIxFWJglcBDRvcWRWUc909KW71yRPZDdc3c7sEEQckAQuAlqz7Sil5+reCi92tOTrpwUb+JQYGqR7diUZMXFKxD9J4CKoM2fP1flnFBXbh84lYhVCf157eLBze9Kc1RH7tiMShyRw4ZP7NPv5//62zj+vsMT+IWDEuHKrSqnnOf47f8MPJkUirEoSuPCpW4ds53bb5g3r/PNq6n5IAvdUu2TvqNwlPP3OOhmZIkIiCVz4ZLPZGHtzdwD+uWZ/nafVFxWXk56aLAv61nLntV28iortOVTAuBeXseW745wqLGNU7hJnzXYh3Mm7SfiVk53h3P5g+V5+PFXMqNwlrD+P+mVFpeVeN+6EXfeOTXjabZWkGjMWbOGh2StNiEhYhSRw4Vfjhq4bjh+v2sek19YAMHvRN/4e4tfJgjKqiWxxrHjSqmlilxcQ50cSuPCrUcM0MiMwczJ/w0HAnsSFEJEjCVwE9Ozv+vk8Hs7Y8KWbDkUqnLg2fWxffnPDxTx8e08mj+jjdX7upztY+c1hEyITsUoSuAgo1U+p0wkzV4T8M753TKMfdFnLIFcmtqaN6zPoslZ069CEzm0a8dhdnkl82ebDvPWPHYzKXcLhE2dNilLEEkng4rycq6jik1X7wnrMha0aGRNMnOrUuhF5k4b5PPfYG19FORoRiySBi6CeufcqRl6veH5cf4/FGN5fFt6CTP0vaRHp0BLCyxMG+jxeWVXl83g8qK6uZu2Oo3H9O0aCJHARVMsLGjCkV2uyM9No3yLzvAtSJSfJn9v5aJCewm9vuJj+PTw/AJ//2yaTIjLee0t2M+fDbdw7fanZocS0YGtipiil5imlliul1iqlbnI796JSaqzxIYpY84dfXOrcHpW7hIKz5/xO9CmvkBZUJAy8rBX3/KybR5dKvA09LC6tYO2Oo3yz5wSLv/7e7HAsIViTaARwQms9EBgOzFJKNVNKfQrcFPihIlFMeHkF90zL93lu7+GCKEcT/34x+EIA1u2MzUWiq6urqQph5m55RaXHxLD7ZixjzofbePH/NhsdYtwIlsAXAFMd2zagAmgIPAHMMy4sEet8jSgpclust0buuxuiEU5CqbkZXFDs/XrHgnum5TN6Wj5frD8Y8Lrn37Mn6tmLvgnY113XMg7xLGAC11oXaa0LlVKZwEJgitZ6r9ZaboEnuN/c0NXr2P0vLfd7fVYDKSMbKR1bZpodgl/uyfbdf+8KmHx3fX/abfuM3+t2/+D/XKILeldJKdUWyAfmaa3nGx+SiBfuBZimj+1rYiTxJT3VtSjG2dLYaoWfKvScbbthl6tujvv9kFc+2Opx3bN/3ej3Z27dc9Lr2JbvTvDDcddY+OLSCg4eKwo7XqsLuDyKUioHWAzcp7X+IjohCauYcf8AJsxcQd/uLVi97QgAJwtKaZKVzgfLPYcY+psQJOrmDzOWM2vCQDJipFBY/kbPmuazF21l0p29PbrS3po4NGD/fc2N2tx3N7Dr+9N8vGofHVpk8vL79ho8T91zJTMW2Ltfxt1yCSu2HGLzdycAmDl+YEKVLLYF+oqjlHoJ+BWw0+3wDVrrEqXUE8ARrfWcYE9y7FihdGLFseLScu6bYe8+aZ5dn9wxfT1a34+N7EMnmcQTUbXLy44crhjSs7VJ0bjUteyt+9/K0o0/8M5n4a1S1LtLM+679ZI6xRBLmjXLtAU6H6wPfLzWuoXWeojbvxLHuSdCSd4i/rm3/nwNG5TkHXm1a4i/86/YWo7tsZHetVxCuc79b2Vwz1ZhP697l024Xv9oG/M/33XejzeDzKwQEVEzy7KmD7RV0wakpiT5nQou6qZ7xyZex3yNAoom96GDzRrXD3r9DVe180jYtce122wBG59+lZRVhD1y5UxRGWu2H+XzdQctNepFEriIiOFXtnNuV1RWcej4WSz0PrCkVx8a7LG/ZIP/YXslZRUUFtd9cepARrvpHHL1AAAI+UlEQVTNBcjysXj1X8Zc7bF/veNvJm/SMPImDfO5qMXtwzoDeJRwCGbci8v8zkvw54FZroUz/jR3XViPNVPAm5hChMq99fRfzy4FZBam0dJSkpk5fqBz+OYHy/dyU/+OPq8d9+IyAMO+ER0/XeJ17M2JQ6murqaopIJjp0rIyc7gid9eQWpKMjnZ9UNqYV93ZTuucyT6Lm0bO4cePjayD8+8sx6A2Q8MorKqOuAw1nDsP1pIVXU1SSHEV11dzfzPv2XgpS1pl+M9vHPv4QKSk2w+z0WCJHAREef7dVfUTcP6KYz6aVfy/rnD4/ipwjLqpyV7DDkEewGyWwddGLHnL6+oori0nEfnrHYeq1moOclmA5uNRg1SaeSYB1CXRDbpzt6cLS3nXHkV2ZlpQT+MSsoqqJ8WPMX5Goq5++AZurRt7PP6Q8fPMuXNrxh7c3fSUpL5Yv1Bvlh/0CueHftO8qyjXo1RH5zShSIM8/y4/maHkBD6qGbO7YLicxSVlPPQ7JX8/oVllJRVeIwM+WTVvrD7eKurq3nq7a+9yiIsXnuAMc8t9eh+APtCzUZpkJ5Cdmaaz3N5k4Yx/Xeu+QY13zqC+cMM75b76x9v83v9lDft8xjnfLjN44Oz5v5PzSLUz0ah2JgkcBExbz461GPf3xtNRJZ7K/O9L3aT9w9XUvGVxELtH66qqqa6upo/z1vP3sOFPPW2Z9/w35bsPs+IjdO0UfCbp6EIdfm/QrdyBg/NXsnaHUe9rrkgy7j3gXShiIhJSnJ1o3Rtn21iJImrZkJVMMG6F0ZPy/dZkGpU7hKyMlIYf9tl5x2j0Sb+uhfT5ttndrr3Zb+/bA/LtxzixfsGADBz4RY27T7u8dgnR13J43lrAdAHTlGvXpLHSJmqqsDfXuZ86N1yn3J36DdgwyUtcBFRM8cP5I5rLuLh23sGv1hETCijNNxbgpNfX+P3uoM/FgWsJlhQXO7VGgd48JeX8dbEoT4eEV2qnavxMHpavvNm+ier9nGm6BwfO1aSqp28Z9w/gLbNGzr3p83f6LxRWqMgzJE8fVQzZ/+/EaQFLiKqYf0Urr2irdlhJBz3xFPbKw8OIj21HhWVVc4RQmfOnmPdzh85eqqY3l2a0fIC1yiicCfDvPrgYCqrqslIj810Mua5pVzcznVDctGyPSyqtZpUWmqyz6GPAGu2H8GGjdc+8t0vPn1sX/6+bA9fbffsPnnm3qs8XlcjBJxKHykylV4I49Wexu5r5MOBo4U88T9fex1/a+JQ50gif9Phh/RqzdJatU78PY/Z9h8p5Mm53r+nP7MfGOTsUnr01VUcP1Ma9DHuv/eZojKPm7mRek3qNJVeCBFf/A3jW7XVu++8W4dsbuzbnkYNUsmbNIyR1ytef2SIwRFGRvsWmdx1vQr5evf7AdN/14/01MDF12rXW2nUMI2xN3cH4InfGtfnXZu0wIWIE3sPFzj7pl9/ZAj1kn23z2rGMdd2VbcchvZq7awcmDu2L819TIn/09yv2XekEICpd19Ox5ZZkfoVIqqqutpjdijYF4j+55r9HD1Zws0DOtKmmb2Lw9c8huNnSnj01dVexyF63zqCtcBjs9NKCBG29m6ta3/JG/yvpfnV9qN8tf0o9dPqkZWR4jN5A/zxN1eEPFPRTLXju6l/Bxqkp3DbkM4hPd7fkMSa6f2xQLpQhIgTSUk2Z12RYCbcdikXZKUza8JAr3MlZRVkBhk5EevJu8Yjd/QCYMClLbl5gO8yA6F6ecJAnvjtFc6p/bFAulCESHD+blrG4s3JaCuvqGTMc1/SIL0eL08YFPXnly4UIYQ4Tyn1kmP6g0xa4EIkuPKKKr7eeZQru+Y4x4mPu+USjxorwhzBWuBBE7hSKgXIAzoAacDTwHZgLlANbAXGaa391g6VBC6EEOGLxDjwEcAJrfVAYDgwC3gBmOI4ZgNurmugQgghwhNKAl8ATHVs24AKoA/wpePYp8BPIh+aEEKIQILexNRaFwEopTKBhcAU4DmtdU23SCEgq9YKIUSUhTQOXCnVFsgH5mmt5wPu/d2ZwGkDYhNCCBFA0ASulMoBFgMTtdZ5jsMblVJDHNs3AJFZjE4IIUTIQhmF8hLwK2Cn2+HxwEwgFdgB3Ku1rvT3M2QUihBChK/OwwgjQRK4EEKET8rJCiFEnIpKC1wIIUTkSQtcCCEsShK4EEJYlCRwIYSwKEngQghhUZLAhRDCoiSBCyGERUkCF0IIi0rIJdXCWaRCKfU4cCP2MroTtNZrlVKdQ702mr9XXSilmgPrgWuxxz+XxH0t/hu4CXupiFewl06eSwK+Ho73ytvY3yuVwL0k6N+HUuoqYJrWekg4v1ckrvUXU6K2wENapEIp1RsYDFwF3A7Mdjw+nGtjnuNN+hpQ4jiUyK/FEKAf0B/779CWBH49gJ8C9bTW/YA/Ac+QgK+HUupR4E0g3XHIqNcgrMVyEjWBh7pIxQBgsda6Wmt9AKinlGoW5rVW8BwwBzjk2E/k1+J64BtgEfAx8AmJ/Xrswh5vEpAFlJOYr8d3wK1u+0a9BmEtlpOQCVxrXaS1Lqy1SIXNxyIVWcAZt4fWHA/n2pimlPoNcExr/Znb4YR8LRyaApcDtwFjgXeBpAR+PYqwd5/sBN7AXoU04f4+tNZ/x/7hVcOo18DXtX4lZAKHkBepKHBs1z4ezrWxbhRwrVJqKdATeAdo7nY+kV4LgBPAZ1rrc1prDZTi+SZKtNfjAeyvRxfgMuz94alu5xPt9ahhVL4Ia7GchEzgYSxSsRK4XimVpJRqh70ldjzMa2Oa1nqQ1nqw1noIsAkYCXyaiK+FwwpguFLKppRqBTQAvkjg1+MUrpbiSSCFBH2v1GLUaxDWYjkJOQoFmAxkA1OVUjV94eOBmUqpmkUqFmqtK5VSy4HV2D/sxjmufQh4I8RrrSic3y+uXgut9SdKqUHAWlyx7yVBXw/gRSDPEX8q9vfOOhL39ahh1HvE69pAQUg5WSGEsKiE7EIRQoh4IAlcCCEsShK4EEJYlCRwIYSwKEngQghhUZLAhRDCoiSBCyGERf0/xyE5cCSxw8UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f8d37d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ios : ios, dylib, nil, uiview, nsstring\n",
      "android : android, activity, imgsrv, 29297, intent\n",
      "html : br, html, span, amp, div\n",
      "javascript : javascript, x20, 3, 125, x30\n",
      "java : println, servlet, spring, hibernate, java\n",
      "jquery : jquery, ajax, ready, span, li\n",
      "c# : xsl, writeline, binding, net, foreach\n",
      "c++ : avrf, c++, std, cout, cpp\n",
      "php : php, x5c, _post, echo, 125\n",
      "python : python, def, 00, py, django\n"
     ]
    }
   ],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# V\n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "\n",
    "\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "        self._word_counter = Counter()\n",
    "    \n",
    "    def filter_vocab(self, n=10000):\n",
    "        top_words = {k for k,v in self._word_counter.most_common(n)}\n",
    "        self._vocab = {k:v for k ,v in self._vocab.items() if k in top_words}\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1,\n",
    "                     update_vocab=True,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy_sum = 0\n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                if n < top_n_train:\n",
    "                    self._word_counter.update(sentence)\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = set()\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "                    \n",
    "                    w_filter = set()\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if (n >= top_n_train and word not in self._vocab) or (not update_vocab):\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            if word in self._vocab:\n",
    "                                if word in w_filter:\n",
    "                                    reg = 0\n",
    "                                else:\n",
    "                                    w_filter.add(word)\n",
    "                                    w_tmp = self._w[tag][self._vocab[word]] \n",
    "                                    reg = lmbda * (gamma * 2 * w_tmp + (1 - gamma) * np.sign(w_tmp))\n",
    "\n",
    "                                self._w[tag][self._vocab[word]] -= -learning_rate*(dLdw - reg) \n",
    "                            \n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    elif sigma > 0.9:\n",
    "                        # тестовый набор, предсказываем тег\n",
    "                        predicted_tags.add(tag)\n",
    "                        \n",
    "                if n >= top_n_train:\n",
    "                    accuracy_sum += len(predicted_tags.intersection(tags)) / len(tags.union(predicted_tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return accuracy_sum / (total - top_n_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263acf0d156e4273bd2ed88af6063a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.59\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1f7d66b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82ba6cc4e1d4a4089c740d796271660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "\n",
    "\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь\n",
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "        self._word_counter = Counter()\n",
    "\n",
    "\n",
    "    def predict_proba_for_tag(self, tag, s):\n",
    "        sentence = s.split(' ')\n",
    "        z = self._b[tag]\n",
    "\n",
    "        for word in sentence:\n",
    "            z += self._w[tag][self._vocab[word]]  \n",
    "\n",
    "        if z < -20:\n",
    "            sigma = 0\n",
    "        elif z > 20:\n",
    "            sigma = 1\n",
    "        else:\n",
    "            sigma = 1/(1 + np.exp(-z))\n",
    "\n",
    "        if sigma < tolerance:\n",
    "            sigma = tolerance\n",
    "\n",
    "        if sigma > (1 - tolerance):\n",
    "            sigma = 1 - tolerance\n",
    "            \n",
    "        return sigma\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, s):\n",
    "        return [(tag, predict_proba_for_tag) for tag in self._tags]\n",
    "        \n",
    "    def filter_vocab(self, n=10000):\n",
    "        top_words = {k for k,v in self._word_counter.most_common(n)}\n",
    "        self._vocab = {k:v for k ,v in self._vocab.items() if k in top_words}\n",
    "    \n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     lmbda=0.0002,\n",
    "                     gamma=0.1,\n",
    "                     update_vocab=True,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        accuracy_sum = 0\n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                if n < top_n_train:\n",
    "                    self._word_counter.update(sentence)\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "                predicted_tags = set()\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "                    \n",
    "                    w_filter = set()\n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if (n >= top_n_train and word not in self._vocab) or (not update_vocab):\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    #assert z >=0, z\n",
    "                    if z < -20:\n",
    "                        sigma = 0\n",
    "                    elif z > 20:\n",
    "                        sigma = 1\n",
    "                    else:\n",
    "                        sigma = 1/(1 + np.exp(-z))\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    \n",
    "                    if sigma > (1 - tolerance):\n",
    "                        sigma = 1 - tolerance\n",
    "                    \n",
    "                    sample_loss += - y * np.log(sigma)\n",
    "                    sample_loss += - (1 - y) * np.log(1 - sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = (y - sigma) # TODO\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:\n",
    "                            \n",
    "                            if (word in w_filter) or (word not in self._vocab):\n",
    "                                reg = 0\n",
    "                            else:\n",
    "                                w_filter.add(word)\n",
    "                                w_tmp = self._w[tag][self._vocab[word]] \n",
    "                                reg = lmbda * (gamma * 2 * w_tmp + (1 - gamma) * np.sign(w_tmp))\n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*(dLdw - reg) \n",
    "                            \n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    elif sigma > 0.9:\n",
    "                        # тестовый набор, предсказываем тег\n",
    "                        predicted_tags.add(tag)\n",
    "                        \n",
    "                if n >= top_n_train:\n",
    "                    accuracy_sum += len(predicted_tags.intersection(tags)) / len(tags.union(predicted_tags))\n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return accuracy_sum / (total - top_n_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
